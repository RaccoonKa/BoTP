{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практика 02. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvOWdHcUWg98"
   },
   "source": [
    "## Задача: классификация твитов по тональности\n",
    "\n",
    "У нас есть выборка из твитов.\n",
    "Нам известна эмоциональная окраска каждого твита из выборки: положительная или отрицательная. Задача состоит в построении модели, которая по тексту твита предсказывает его эмоциональную окраску.\n",
    "\n",
    "Классификацию по тональности используют в рекомендательных системах, чтобы понять, понравилось ли людям кафе, кино, etc.\n",
    "\n",
    "Скачать выборку можно по ссылкам или же воспользовать уже скачанными файлами ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GYvTkhGnWg98"
   },
   "outputs": [],
   "source": [
    "# если вы работете в GoogleCollab, где работает wget, то данные можно скачать данные так:\n",
    "#%%capture\n",
    "#!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
    "#!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IX-AeH8RWg9-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # библиотека для удобной работы с датафреймами\n",
    "import numpy as np # библиотека для удобной работы со списками и матрицами\n",
    "\n",
    "# библиотека, где реализованы основные алгоритмы машинного обучения\n",
    "from sklearn.metrics import * \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVsSSqFKbAWL"
   },
   "source": [
    "Откроем файлы и создадим массив из текстов и правильных меток для твитов.\n",
    "Сначала идут положительные твиты, потом отрицательные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aBrSNCKVWg9_"
   },
   "outputs": [],
   "source": [
    "# загружаем положительные твиты\n",
    "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = ['positive'] * len(positive) # устанавливаем метки\n",
    "\n",
    "# загружаем отрицательные твиты\n",
    "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = ['negative'] * len(negative) # устанавливаем метки\n",
    "\n",
    "# соединяем вместе\n",
    "df = positive.append(negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfjWsULtaGtf"
   },
   "source": [
    "Посмотрим на полученные данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 2712,
     "status": "ok",
     "timestamp": 1595572083673,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "aqudQEvUWg-D",
    "outputId": "d267f796-b4de-45e9-a350-fd275dc81769"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15931</th>\n",
       "      <td>RT @Blawar_1337: Теперь у нас с @Wake_UA появи...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59532</th>\n",
       "      <td>с днём рождения зайка*))) ухх погуляем мы сего...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47185</th>\n",
       "      <td>RT @Shumkova0406199: @ann_safina Вов вов вов А...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42002</th>\n",
       "      <td>Надо выдернуть звуковую дорожку из \"Доктора Ка...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109035</th>\n",
       "      <td>@_hassliebe_ может все таки на этой неделе вер...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     label\n",
       "15931   RT @Blawar_1337: Теперь у нас с @Wake_UA появи...  positive\n",
       "59532   с днём рождения зайка*))) ухх погуляем мы сего...  positive\n",
       "47185   RT @Shumkova0406199: @ann_safina Вов вов вов А...  negative\n",
       "42002   Надо выдернуть звуковую дорожку из \"Доктора Ка...  positive\n",
       "109035  @_hassliebe_ может все таки на этой неделе вер...  negative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 226834 entries, 0 to 111922\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    226834 non-null  object\n",
      " 1   label   226834 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubV1-2JjaZuC"
   },
   "source": [
    "Разбиваем данные на обучающую и тестовую выборки с помощью функции ```train_test_split()``` из **sklearn**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8RscvM_TWg-F"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zpq4QOU5Wg-H"
   },
   "source": [
    "## Baseline: классификация необработанных n-грамм\n",
    "\n",
    "* Сначала попробуем получить преобразование предложений в численный вектор, с которым может работать стандартный алгоритм машинного обучения, например логистическая регрессия. \n",
    "* Для этого нам понадобится познакомиться с понятием n-gram - самых мелких элементов предложения, с которыми можно работать. \n",
    "* Подсчитав количество этих n-грам в предложениях, мы получим искомые численные представления."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quiUoyqNb3WA"
   },
   "source": [
    "Функция для работы с n-граммами реализована в библиотке **nltk** (Natural Language ToolKit), импортируем эту функцию: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hcrWxBzzWg-K"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ib-zYTvfQq5"
   },
   "source": [
    "Прежде чем получать n-граммы, нужно разделить предложение на отдельные слова.  Для этого используем метод ```split()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1052,
     "status": "ok",
     "timestamp": 1595572083675,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "A2Ql8Em4Wg-N",
    "outputId": "94b828c7-4276-4bf2-a172-98f03ee4e786"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Кому', 'нужен', 'ломтик', 'июльского', 'неба?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Кому нужен ломтик июльского неба?'.split()\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6V5P2Jcc4Oy"
   },
   "source": [
    "Чтобы получить n-грамму для такой последовательности, используем функцию ```ngrams()```. \n",
    "\n",
    "На вход подается два параметра:\n",
    "* список с разделенным на отдельные слова предложением (у нас он хранится в переменной ```sentence```);\n",
    "* параметр n, определяющий, какой тип n-грамм мы хотим получить.\n",
    "\n",
    "\n",
    "Чтобы полученный объект отобразить, делаем из него ```list```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1595572083675,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "F9oqpykUc5e9",
    "outputId": "38c2dd15-5830-4da0-c37a-3d72f1170dd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Кому',), ('нужен',), ('ломтик',), ('июльского',), ('неба?',)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sentence, 1)) # униграммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZKRhRlxfoj4"
   },
   "source": [
    "Аналогично мы можем получить биграммы - для этого заменяем параметр **n** в функции **ngrams** с 1 на 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1595572083991,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "Bzl6t5dpWg-P",
    "outputId": "02d05cbb-e64c-4c3c-d5ae-681c63e0d0cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Кому', 'нужен'),\n",
       " ('нужен', 'ломтик'),\n",
       " ('ломтик', 'июльского'),\n",
       " ('июльского', 'неба?')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sentence, 2)) # биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1595572084259,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "nCkkFzWLWg-R",
    "outputId": "34818c20-192b-4786-fcc6-149ea0017726"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Кому', 'нужен', 'ломтик'),\n",
       " ('нужен', 'ломтик', 'июльского'),\n",
       " ('ломтик', 'июльского', 'неба?')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sentence, 3)) # триграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1595572084260,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "GygS6_fJWg-S",
    "outputId": "17d4a687-d6ac-4982-c444-aa5a77de9337"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Кому', 'нужен', 'ломтик', 'июльского'),\n",
       " ('нужен', 'ломтик', 'июльского', 'неба?')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sentence, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JewKs4XU-so"
   },
   "source": [
    "### Векторизаторы\n",
    "\n",
    "Векторизатор преобразует слово или набор слов в числовой вектор, понятный алгоритму машинного обучения, который привык работать с числовыми табличными данными.\n",
    "\n",
    "Ниже - пример преобразования слов в двумерных вектор, каждому слову соответствует точка на плоскости.\n",
    "\n",
    "![Рисунок](img/download.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5hiNv2eVAc-"
   },
   "source": [
    "На начальном этапе нам будет достаточно тех инструментов, которые уже есть в знакомой нам библиотеке **sklearn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cPplZnxeVEBR"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # можно заменить на любой другой классификатор\n",
    "from sklearn.feature_extraction.text import CountVectorizer # модель \"мешка слов\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBN16KYZWg-U"
   },
   "source": [
    "Самый простой способ извлечь признаки из текстовых данных -- векторизаторы: `CountVectorizer` и `TfidfVectorizer`\n",
    "\n",
    "Объект `CountVectorizer` делает следующее:\n",
    "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` -- количество слов или n-грам во всем корпусе\n",
    "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ\n",
    "\n",
    "![Рисунок](img/download2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oklbwY_vWg-X"
   },
   "source": [
    "На рисунке пример векторизации для униграмм, но можно использовать любые n-граммы. Для этого у объекта ```CountVectorizer()``` есть параметр **ngram_range**, который отвечает за то, какие n-граммы мы используем в качестве признаов:<br/>\n",
    "ngram_range=(1, 1) -- униграммы<br/>\n",
    "ngram_range=(3, 3) -- триграммы<br/>\n",
    "ngram_range=(1, 3) -- униграммы, биграммы и триграммы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLTEzNWxrx5X"
   },
   "source": [
    "Инициализируем ```CountVectorizer()```, указав в качестве признаков униграммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hWKQcLfBWg-W"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-gFUNm6huAz"
   },
   "source": [
    "После инициализации _vectorizer_ можно обучить на наших данных. \n",
    "\n",
    "Для обучения используем обучающую выборку ```x_train```, но в отличие от классификатора мы используем метод ```fit_transform()```: сначала обучаем наш векторизатор, а потом сразу применяем его к набору данных. Это похоже на то, как работает label encoder и one-hot-encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TkJESwGfhq4v"
   },
   "outputs": [],
   "source": [
    "vectorized_x_train = vectorizer.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPJDPt7xvWKe"
   },
   "source": [
    "Так как результат не зависит от порядка слов в текстах, то говорят, что такая модель представления текстов в виде векторов получается из *гипотезы представления текста как мешка слов*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyT1Kz6Ppt8n"
   },
   "source": [
    "В vectorizer.vocabulary_ лежит словарь, отображение слов в их индексы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "executionInfo": {
     "elapsed": 1849,
     "status": "ok",
     "timestamp": 1595572093192,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "72knqTvCWg-Y",
    "outputId": "2b67a368-df3b-45ff-c17a-3cf25252acac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rusheeeeeeeeer', 74695),\n",
       " ('его', 129725),\n",
       " ('так', 219184),\n",
       " ('присобачила', 194320),\n",
       " ('что', 237136),\n",
       " ('хрен', 234005),\n",
       " ('он', 173185),\n",
       " ('отвалится', 175080),\n",
       " ('dyyybs', 27504),\n",
       " ('ну', 169108)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectorizer.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdKqMyRKjA-p"
   },
   "source": [
    "В нашей выборке 170125 текстов (твитов), в них встречается 243760 разных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 11667,
     "status": "ok",
     "timestamp": 1595571716091,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "IjOD7nrsi2fc",
    "outputId": "5141f2d9-956c-49cc-fda4-8ed311ee8b04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170125, 243193)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7geX_YHDjG9v"
   },
   "source": [
    "Так как теперь у нас есть **численное представление** и набор входных признаков, то мы можем обучить модель логистической регрессии (или любую другую)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "executionInfo": {
     "elapsed": 40995,
     "status": "ok",
     "timestamp": 1595571745425,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "DCdROyxsWg-b",
    "outputId": "96fb55d1-fac0-4ded-b006-517701d0bdcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=88)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression(random_state=88, max_iter=1000) # фиксируем random_state для воспроизводимости результатов\n",
    "clf.fit(vectorized_x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Z9iIlzOjVUF"
   },
   "source": [
    "С тестовыми данными нужно проделать то же самое, что и с данными для обучения: сделать из текстов вектора, которые можно передавать в классификатор для прогноза класса объекта. \n",
    "\n",
    "У нас уже есть обученный векторизатор ```vectorizer```, поэтому используем метод ```transform()``` (просто применить его), а не ```fit_transform``` (обучить и применить)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "u2_AW8FpjWHQ"
   },
   "outputs": [],
   "source": [
    "vectorized_x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahCRPAeWjtcl"
   },
   "source": [
    "Как раньше, для получения прогноза у обученного классификатора используем метод ```predict()```.\n",
    "\n",
    "С помощью функции ```classification_report()```, которая считает сразу несколько метрик качества классификации, посмотрим на то, насколько хорошо мы предсказываем положительную или отрицательную тональность твита ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "executionInfo": {
     "elapsed": 43650,
     "status": "ok",
     "timestamp": 1595571748094,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "juvxbzinWg-d",
    "outputId": "1d876772-e00b-4788-8a22-a365957b1f1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.77      0.76     28068\n",
      "    positive       0.77      0.76      0.77     28641\n",
      "\n",
      "    accuracy                           0.76     56709\n",
      "   macro avg       0.76      0.76      0.76     56709\n",
      "weighted avg       0.76      0.76      0.76     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yiLk1P_xYQ2"
   },
   "source": [
    "## Использование триграмм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjy5ZPmwWg-j"
   },
   "source": [
    "Попробуем сделать то же самое, используя в качестве признаков триграммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "executionInfo": {
     "elapsed": 77463,
     "status": "ok",
     "timestamp": 1595571781915,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "hmQHqUpRWg-k",
    "outputId": "1777b480-9cbe-40e4-c2a6-d67fba32cb1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.78      0.78     28068\n",
      "    positive       0.78      0.78      0.78     28641\n",
      "\n",
      "    accuracy                           0.78     56709\n",
      "   macro avg       0.78      0.78      0.78     56709\n",
      "weighted avg       0.78      0.78      0.78     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# инициализируем векторайзер \n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "# обучаем его и сразу применяем к x_train\n",
    "trigram_vectorized_x_train = trigram_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=88, max_iter=1000)\n",
    "clf.fit(trigram_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторизатор к тестовым данным\n",
    "trigram_vectorized_x_test = trigram_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(trigram_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MonLW7AyWg-m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlWxW3e9Wg-m"
   },
   "source": [
    "## TF-IDF векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7hCxZRtWg-m"
   },
   "source": [
    "`TfidfVectorizer` делает то же, что и `CountVectorizer`, но в качестве значений выдает **tf-idf** каждого слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Fv7DfTkJWg-n"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02f_zZm14PHM"
   },
   "source": [
    "Действуем аналогично, как с ```CountVectorizer()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "executionInfo": {
     "elapsed": 203173,
     "status": "ok",
     "timestamp": 1595571907637,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "-rMYiobAWg-p",
    "outputId": "e7782d5f-d23e-4515-ddac-b80de2018e49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.76      0.76     28068\n",
      "    positive       0.76      0.76      0.76     28641\n",
      "\n",
      "    accuracy                           0.76     56709\n",
      "   macro avg       0.76      0.76      0.76     56709\n",
      "weighted avg       0.76      0.76      0.76     56709\n",
      "\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# инициализируем векторизатор, в качестве переменных используем униграммы\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "# обучаем его и сразу применяем к x_train\n",
    "tfidf_vectorized_x_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=88, max_iter=1000)\n",
    "clf.fit(tfidf_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторизатор к тестовым данным\n",
    "tfidf_vectorized_x_test = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(tfidf_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkuods5LWg-q"
   },
   "source": [
    "Результат не улучшился, поэтому вернемся к `CountVectorizer()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D39SSh0zWg-r"
   },
   "source": [
    "## Токенизация\n",
    "\n",
    "Токенизировать - значит, поделить текст на части: слова, ключевые слова, фразы, символы и т.д., иными словами **токены**.\n",
    "\n",
    "Самый простой способ токенизировать текст - разделить с помощью функции `split()`. Но `split` упускает очень много всего, например, не отделяет пунктуацию от слов. Кроме этого, есть еще много менее тривиальных проблем, поэтому лучше использовать готовые токенизаторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hoSe08N2Wg-r"
   },
   "outputs": [],
   "source": [
    "import nltk # уже знакомая нам библиотека nltk\n",
    "from nltk.tokenize import word_tokenize # готовый токенизатор библиотеки nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiDt3L8Y8god"
   },
   "source": [
    "Чтобы использовать токенизатор ```word_tokenize```, нужно сначала скачать данные для nltk о пунктуации и стоп-словах. Это просто требование nltk, поэтому просто скачаем требумую информацию:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "executionInfo": {
     "elapsed": 203991,
     "status": "ok",
     "timestamp": 1595571908471,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "gPH3yMcumsdd",
    "outputId": "977038f7-c30f-4676-86cf-042f32f097be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SNTkachenko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SNTkachenko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NfDb8D_9DqD"
   },
   "source": [
    "Применим токенизацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 203985,
     "status": "ok",
     "timestamp": 1595571908471,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "zrJDGpgYWg-4",
    "outputId": "8922d9ce-a200-4f9b-9f00-2ec936638f5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':', '(']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = 'Но не каждый хочет что-то исправлять:('\n",
    "word_tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxy7KGZI9bhK"
   },
   "source": [
    "Если использовать просто ```split()```, то знаки пунктуации :( не отделяются от слова \"исправлять\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 203980,
     "status": "ok",
     "timestamp": 1595571908472,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "p52dIuSI9W6o",
    "outputId": "80080cf0-6165-4fd2-f4b8-adf2ba709f29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять:(']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_702Dg5OWg-5"
   },
   "source": [
    "В nltk вообще есть довольно много токенизаторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "executionInfo": {
     "elapsed": 203974,
     "status": "ok",
     "timestamp": 1595571908472,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "Ps8oPYoTWg-6",
    "outputId": "edcce30e-ce55-468b-8adc-3daa972e532b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BlanklineTokenizer',\n",
       " 'LineTokenizer',\n",
       " 'MWETokenizer',\n",
       " 'NLTKWordTokenizer',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'SExprTokenizer',\n",
       " 'SpaceTokenizer',\n",
       " 'StanfordSegmenter',\n",
       " 'SyllableTokenizer',\n",
       " 'TabTokenizer',\n",
       " 'TextTilingTokenizer',\n",
       " 'ToktokTokenizer',\n",
       " 'TreebankWordTokenizer',\n",
       " 'TweetTokenizer',\n",
       " 'WhitespaceTokenizer',\n",
       " 'WordPunctTokenizer',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_treebank_word_tokenizer',\n",
       " 'api',\n",
       " 'blankline_tokenize']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "dir(tokenize)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmnGCL5iWg-8"
   },
   "source": [
    "Они умеют выдавать индексы в строке для начала и конца каждого слова-токена:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 203968,
     "status": "ok",
     "timestamp": 1595571908473,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "Jejj5X7QWg-8",
    "outputId": "21214ea3-959d-4b50-fa26-30f606b883fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (3, 5), (6, 12), (13, 18), (19, 25), (26, 38)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_tok = tokenize.WhitespaceTokenizer()\n",
    "list(wh_tok.span_tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-wf6A1EWg--"
   },
   "source": [
    "Некторые токенизаторы ведут себя специфично:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 203963,
     "status": "ok",
     "timestamp": 1595571908474,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "2REwpHGWWg-_",
    "outputId": "848159bf-477b-415c-cc4f-db2eaefd9265"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', \"n't\", 'stop', 'me']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tckre90JWg_B"
   },
   "source": [
    "А некоторые -- вообще не для текста на естественном языке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 203956,
     "status": "ok",
     "timestamp": 1595571908474,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "F1Ml3xtaWg_D",
    "outputId": "2ec44cac-37c0-409c-b00b-32573d0fb6a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(a (b c))', 'd', 'e', '(f)']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.SExprTokenizer().tokenize(\"(a (b c)) d e (f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM2kvAo0_b93"
   },
   "source": [
    "**Подходящий токенизатор подбирается исходя из требований задачи!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhVrgkSaWg_K"
   },
   "source": [
    "## Стоп-слова и пунктуация\n",
    "\n",
    "**Стоп-слова** - это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе. Для модели это просто шум. А шум нужно убирать. По аналогичной причине убирают и пунктуацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "executionInfo": {
     "elapsed": 203951,
     "status": "ok",
     "timestamp": 1595571908475,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "Ld-h6WKyWg_K",
    "outputId": "090c952d-213a-434b-8d76-d5ee08f0eece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "# импортируем стоп-слова из библиотеки nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# посмотрим на стоп-слова для русского языка\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ihn2Y_SzBU57"
   },
   "source": [
    "*Знаки* пунктуации лучше импортировать из модуля **String**. В нем хранятся различные наборы констант для работы со строками (пунктуация, алфавит и др.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 203944,
     "status": "ok",
     "timestamp": 1595571908475,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "x64U3FdPWg_M",
    "outputId": "d255d6db-84d9-4517-da65-7b79a23a87b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9LGTLaEBwsC"
   },
   "source": [
    "Объединим стоп-слова и знаки пунктуации вместе и запишем в переменную ```noise```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "VfH5y50wWg_O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "noise = stopwords.words('russian') + list(punctuation)\n",
    "print(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3gweXaWWg_P"
   },
   "source": [
    "Теперь нужно обучить нашу модель с учетом новых знаний про токенизацию и стоп-слова. \n",
    "\n",
    "Для этого мы можем собрать новый векторизатор, передав ему на вход:\n",
    "* какие n-граммы нам нужны, параметр **ngram_range**;\n",
    "* какой токенизатор мы используем, параметр **tokenizer**;\n",
    "* какие у нас стоп-слова, параметр **stop_words**.\n",
    "\n",
    "*Напоминание:* мы используем готовый токенизатор ```word_tokenize```, а стоп-слова хранятся в переменной ```noise```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "fbXrVeRRuAxx"
   },
   "outputs": [],
   "source": [
    "# инициализируем умный векторайзер \n",
    "smart_vectorizer = CountVectorizer(ngram_range=(1, 3), \n",
    "                                   tokenizer=word_tokenize, \n",
    "                                   stop_words=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "executionInfo": {
     "elapsed": 252716,
     "status": "ok",
     "timestamp": 1595571957270,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "7Nc6D-nwWg_P",
    "outputId": "22ab3aef-053b-447b-b784-c46f990353db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.82      0.79     28068\n",
      "    positive       0.81      0.75      0.78     28641\n",
      "\n",
      "    accuracy                           0.78     56709\n",
      "   macro avg       0.79      0.79      0.78     56709\n",
      "weighted avg       0.79      0.78      0.78     56709\n",
      "\n",
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# обучаем его и сразу применяем к x_train\n",
    "smart_vectorized_x_train = smart_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=88, max_iter=1000)\n",
    "clf.fit(smart_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторайзер к тестовым данным\n",
    "smart_vectorized_x_test = smart_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(smart_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYWB1foQWg_T"
   },
   "source": [
    "Результат стал немного лучше: accuracy выше, а также заметно подрос recall у негативного класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическое задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tВыполните исследование величины n в n-граммах на результаты (как меняется результаты обучения одной и той же модели, при изменении n, значение можно варьировать от 1 до 5).\n",
    "2.\tВыполните обучение другой модели машинного обучения. Показать изменится ли от этого результат по сравнению с логистической регрессией.\n",
    "3.\tИсследуйте влияние стоп слов на результаты классификации, если все сделать правильно, то можно получить абсолютную точность, т.е. значения всех метрик будут равны 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VlWxW3e9Wg-m",
    "D39SSh0zWg-r",
    "rhVrgkSaWg_K",
    "XsRf9T_SWg_U",
    "ylKZG2MwWg_f",
    "9hedBdcYWhAH",
    "JrqW55jgWhAR",
    "5QYTwyMtWhAZ",
    "DbJrUpARWhAd",
    "MI18l-l9WhAk",
    "1wrEGqBSWhAr",
    "gStgBJy2WhAx"
   ],
   "name": "Копия блокнота \"seminar_1_preprocessing.ipynb\"",
   "provenance": [
    {
     "file_id": "1TtILmuSoWXOYmbTIAQmGaScvuHGWvpsI",
     "timestamp": 1595574229609
    },
    {
     "file_id": "1EdBdyqxLu-WiLmriWNwYl5Ct33JYcEG2",
     "timestamp": 1582113683695
    },
    {
     "file_id": "10_Aehfbxgr3fxXPgI1gM5BTU8yOy-Z4U",
     "timestamp": 1579514615233
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
